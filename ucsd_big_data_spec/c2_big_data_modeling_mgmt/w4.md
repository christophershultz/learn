# Working with Data Models

**Data Model vs. Data Format**
* Learners often assume that the format of the data is the same as the logical model of the data. It is not. 
* Format is a serialized representation of the data. E.g. a CSV. 
* Model is the logical structure / ordering of the data. E.g. relational or graph. 

CSV does not mean we're talking about a relational _model_. That might be a sensible way to organize it, but it could also be represented as a graph or some other model. 

## Streaming Data

One of the challenges with data systems is the velocity at which data comes in. We may thus want to process data as it is generated (stream). This term refers to a constant stream of data flowing from a source (e.g. a sensor, social media, etc.). Any sensor network or IoT environment is typically a streaming data system.

A **stream** is defined as a possibly unbounded sequence of data records that may or may not be related to each other. Each data is typically timestamped and often geo-tagged. Data can stream from many sources.

Streaming data sometimes get referred to as _event_ data, as each data point is often referred to as an event. 

Such systems are designed to manage one record at a time, or a small time window. The computations are done in near-real-time, and independently of other computations. The processing components often subscribe to a system non-interactively, which means they send nothing back to the source, nor do they interact with the source in any way. 

**Dynamic Steering**: Dynamically changing the next steps or direction of an application through a continuous computational process using streaming. This if often a part of data management systems. E.g. a self-driving car uses dynamic steering. 

Streaming Systems: 

* Amazon Kinesis
* Apache Storm
* Spark Streaming
* Apache Kafka
* Flink

## Why is Streaming Data Different? 

**Data at Rest** refers to mostly static data collected from one or more sources. Analysis happens after data is collected. Batch / static. 

**Data in Motion** refers to a situation where the data is analyzed as it is being generated. E.g. sensors from planes, self-driving cars. Stream. 

**Data Processing Algorithms**: 
* Static/Batch: size determines time/space. 
* Streaming: unbounded size, but finite time/space. 

**Streaming Data Management/Processing**: 
* Compute one data element or a small window at a time.
* Relatively fast and simple computations.
* No interactions with the data source.

These are different than batch processing, where analytical steps have access to all data and have more time to complete analytical tasks.

**Lambda Architecture**: most orgs use a hybrid architecture, sometimes referred to a _lambda_ architecture, for processing streaming and batch jobs at the same time. A streaming view of the real-time data is kept and managed until those elements are pushed to a batch system. 

There are two main challenges that need to be overcome: 
* the **size** of the data 
* the **frequency** of the data

Both can change over time. These changes can be unpredictable and may be driven by human behavior. E.g. increases during holidays, sports matches, or major news events. These changes can be periodic or sporadic. 

## Data Lakes

* Data lakes enable batch processing of streaming data. 
* There is a difference between schema-on-write and schema-on-read. 
* We can organize data streams, data lakes, and data warehouses on a spectrum of big data management and storage.

If you think of a _data mart_ as a store of bottled water - cleansed and packaged and structured for easy consumption - the data lake is a large body of water in a more natural state. The contents of hte data lake stream in from a source to fill the lake, and various users of the lake can come to examine, dive in, or take samples.

A data lake: 
* Stores raw data (big)
* Adds data model on read (schema-on-read), only when needed.
* collects all data for current and future analysis.
* infrastructure components which evolve over time based on app-specific needs.

A data warehouse: 
* Stores raw data (big)
* Adds data model on write (schema-on-write), which means we transform and structure the data before load. 
* Often utilizes less of a _raw_ form and builds in hierarchical structure.

